data = [("2010-01-02",500),("2010-02-03",1000),("2010-03-04",1000),("2010-04-05",1000),("2010-05-06",1000),("2010-06-07",1000),("2010-07-08",1000),("2010-08-09",1000),("2011-10-10",1000),("2011-01-02",500),("2011-02-03",1000),("2011-03-04",1000),("2011-04-05",1000),("2011-05-06",1550),("2011-06-07",1100),("2011-07-08",1100),("2011-08-09",1000)]
df = spark.createDataFrame(data,["sale_date","sale_amt"])
df.display()

from pyspark.sql.functions import year,month,quarter,sum

df_col = df.withColumn("year", year(df.sale_date)).withColumn("month",month(df.sale_date)).withColumn("quarter",quarter(df.sale_date))

df_col.groupBy("quarter").agg(sum("sale_amt").alias("Quarterly_tot_sale")).display()
df_col.groupBy("year").agg(sum("sale_amt").alias("Yearly_tot_sale")).display()
